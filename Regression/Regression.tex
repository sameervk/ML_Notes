\documentclass[a4paper, 12pt]{report}

\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{lipsum} %for random text: use as \lipsum[num1 or num1-num2]

\usepackage{color} %can change the background of the text also


\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\setlength{\parindent}{0in}


\begin{document}
\title{Notes on Classification}
\author{Sameer Kesava PhD}
\date{} % or {Month year}
\maketitle

\pagenumbering{roman}
\tableofcontents
\newpage

\pagenumbering{arabic}

%\setcounter{section}{0}

%-----------------------------------
\chapter{Improving the Accuracy}
A robust and accurate model can be achieved by tackling the \textbf{bias-variance} problem.

\begin{eqnarray}
Bias & = & <\hat{f}(X)> - f(X)\\
Variance & = & <\hat{f}(X){^{2}}> - <\hat{f}(X)>{^{2}}\\
<[f(X) - \hat{f}(X)]{^{2}}> & = & Bias{^{2}} + Variance
\end{eqnarray}


%%-----------------
\section{Weights}
\label{sec:weights}
\begin{enumerate}
\item Sample or Class weights can be used to improve the prediction accuracy, e.g. assigning large penalties to wrong predictions of the minority class using the 'class weight' parameter.
\item Alternatively, a user-defined function can be supplied to compute the weights.
\item Weighted micro and macro-averaging useful for scoring imbalanced datasets.
\end{enumerate}

\subsection{ML}

\subsection{DL}
\begin{enumerate}
\item A good starting values for the initializers are Glorot-Xavier or Orthogonal for the weights.
\item For the bias, zeros suffice.
\end{enumerate}

%%----------------

\section{Cross Validation}
\begin{itemize}
\item Can also use \textbf{\textit{Bootstrap}}.
\item Once the best model is chosen through CV, then the model should be trained on the whole dataset for the final model.
\end{itemize}
\begin{enumerate}
\item Use k-fold CV. k = 10 is a good starting value.
\item Use stratified k-fold to preserve class proportions.
\end{enumerate}

%%----------------

\section{Regularization}
\label{sec:regularization}
\subsection{LASSO or L1-norm}
A useful feature selection technique
\subsection{Ridge Regression or L2-norm}

\subsection{L1-L2 norm}
\begin{itemize}
\item Implemented as ElasticNet in {\color{red}scikit-learn}.
\end{itemize}
%%----------------
\section{Brute-Force}
\label{sec:bruteforce}

Brute-force algorithm is effective for small datasets.\\
Query time grows as O[pn], where p is the number of features/predictors/dimensions and n is no. of data points.
%%----------------

\section{Small Dataset}
Also see \nameref{sec:augmentdata} \autoref{sec:augmentdata}.

\label{sec:smalldatasize}
n: number of samples\\
p: number of features/predictors


When p $>$ n, then there is no unique solution - variance is $\infty$.
\subsection{When p is comparable to n}
\begin{enumerate}

\item Use shrinkage methods such as Lasso or L1 norm with k-fold CV for feature selection.
%\item Check \hyperref[bruteforce]{\ref{bruteforce}}.
\item Check \nameref{sec:bruteforce} \autoref{sec:bruteforce}.

\end{enumerate}

\subsection{p ${\ll}$ n}
\begin{enumerate}
\item Use k-fold CV.
\end{enumerate}


%%----------------
\section{Class Imbalance}
Class imbalances can be handled using the following approaches.
\begin{itemize}
\item Upsampling.
\item Downsampling.
\item Generation of synthetic training samples using methods such as SMOTE.
\item Data augmentation; see \autoref{sec:augmentdata}. 
\end{itemize}

%%----------------

\section{Ensemble Learning}
Random Forest, for example, is a type of ensemble learning method as different trees train on {\color{red}different subsets of the samples(?)}.

\begin{itemize}
\item Use Nested CV for ensemble learning.
\item Boosting
\item Bagging
\item Stacking
\item Majority/Plurality voting for multiclass setting.
\item Cloning estimators ({\color{cyan}sklearn.base import clone}) for fitting on different formats of training data.
\end{itemize}

\subsection{Bagging}
\label{subsec:bagging}
\begin{itemize}
\item Used in Random Forests, random feature subsets are selected with replacement.
\item Improves accuracy of unstable models.
\item Reduces overfitting by reducing variance.
\item Ineffective in reducing model bias. Hence, should be used in conjunction with ensemble classifiers with low bias such as unpruned decision trees, i.e. where max$\_$depth = 0.
\end{itemize}

\subsection{Boosting}
\label{subsec:boosting}
\begin{itemize}
\item Adaptive Boosting ({\color{cyan}from sklearn.ensemble import AdaBoostClassifier})
\begin{enumerate}
\item Focus on samples that are hard to classify.
\item Can lead to decrease in both bias and variance.
\item Uses an ensemble of classifiers: the i+1th learner has it's input the output of the ith learner.
\end{enumerate}
\end{itemize}

\subsection{Stacking}
\begin{itemize}
\item For reducing bias.
\item Uses different learners and then a meta-learner in the end which takes the output of different learners to  yield the response.
\end{itemize}

%%----------------
\section{Augmenting Data}
\label{sec:augmentdata}
Accuracy can be improved by augmenting to the data and then fitting the model with both the original and augmented datasets. Some examples are

\begin{itemize}
\item Adding random noise to the data.
\item In case of images, random rotations of the images.
\end{itemize}

%%----------------
\section{Hyperparameter Tuning}
\begin{itemize}
\item Hyperparameter optimization based on Gaussian Process Regression and Bayesian Optimization
\item keras tuner in keras
\item GridSearchCV or RandomSearchCV in scikit-learn
\item RandomSearch can be used as the baseline against which optimization algorithms can be evaluated.
\end{itemize}

%%----------------
\section{Step Functions}
\begin{itemize}
\item Instead of a single function fitting to all the data, i.e. global fitting, step function approach allows local fitting.
\item Popular in biostatistics and epidemiology. 
\item Wavelet or Fourier series can be used to construct basis functions in spline fitting.
\item CV can be used to determine the number of knots.
\item RandomForest regression is like spline fitting, i.e. it is a sum of piecewise functions (linear in this case).
\end{itemize}
%---------------------------
\chapter{ML Methods}

%%--------------------------
\section{Linear Regression}
Can be used as the \textbf{\textit{base model}}.

%%--------------------------
\section{Principal Coefficient Regression}
\label{sec:pcr}
\begin{itemize}
\item Works well when the variance can be explained by the first few principal components.
\item Disadvantage is that PCR does not account for response variable Y.
\end{itemize}

%%--------------------------
\section{Partial Least Squares}
\begin{itemize}
\item In contrast to PCR, uses response variable Y.
\end{itemize}

%%--------------------------
\section{Generative Additive Models}
\begin{itemize}
\item For multivariate data, GAMs can be used for regression.
\item For more general models, random forest or xgboost regressors should be explored.

\end{itemize}

%-----------------------------
\chapter{DL Methods}
All operations, e.g. loss functions, activation functions, in neural networks must be differentiable.

%-----------------------------
\chapter{Loss Functions}
The target could be a scalar such as weather prediction, or a vector such as the coordinates of bounding box in object detection.


%%----------------
\section{MAE}
\begin{itemize}
\item Can be used if there are not many outliers present.
\end{itemize}

%%-----------------
\section{MSE}


%%------------------
\section{MSLE}
\begin{itemize}
\item Mean-Squared Log Error
\item Used when Y is large.
\end{itemize}

%-----------------------------
\chapter{Statistical Tests}
%%----------------
\section{T-statistic}
When you run a hypothesis test, you use the T statistic with a p value. 

\begin{align}
	t-statistic = \frac{\hat{\beta}}{SE(\hat{\beta})}
\end{align}

$\hat{\beta}$ is the coefficient estimate from the fitting.

%%----------------
\section{Z-statistic or Z-score}
\paragraph{statisticshowto.com:} Simply put, a z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is.

A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ.

Z-scores are a way to compare results to a “normal” population. Results from tests or surveys have thousands of possible results and units; those results can often seem meaningless. For example, knowing that someone’s weight is 150 pounds might be good information, but if you want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming (especially if some weights are recorded in kilograms). A z-score can tell you where that person’s weight is compared to the average population’s mean weight.
%%----------------
\section{Confidence Interval}
For normally distributed error, 95\% confidence interval corresponds to 2$\sigma$.


\end{document}